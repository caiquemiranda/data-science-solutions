{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data, Machine Learning e Deep Learning Estruturas de aprendizagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este capítulo apresenta cuidadosamente a estrutura de big data usada para processamento paralelo de dados, chamada Apache Spark. Ele também abrange diversas estruturas de aprendizado de máquina (ML) e aprendizado profundo (DL) úteis para a construção de aplicativos escalonáveis. Depois de ler este capítulo, você entenderá como o big data é coletado, manipulado e examinado usando tecnologias resilientes e tolerantes a falhas. Ele discute as estruturas Scikit-Learn, Spark MLlib e XGBoost. Também cobre uma estrutura de aprendizado profundo chamada Keras. Conclui discutindo formas eficazes de estabelecer e gerenciar essas estruturas.\n",
    "\n",
    "Estruturas de big data oferecem suporte ao processamento paralelo de dados. Eles permitem que você contenha big data em muitos clusters. A estrutura de big data mais popular é o Apache Spark, que é construído na estrutura Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data significa coisas diferentes para pessoas diferentes. Neste livro, definimos big data como grandes quantidades de dados que não podemos manipular e manipular adequadamente usando métodos clássicos. Devemos, sem dúvida, utilizar estruturas escaláveis e tecnologias modernas para processar e extrair informações destes dados. Normalmente consideramos os dados “grandes” quando eles não cabem no espaço de armazenamento atual na memória. Por exemplo, se você possui um computador pessoal e os dados à sua disposição excedem a capacidade de armazenamento do seu computador, trata-se de big data. Isto se aplica igualmente a grandes corporações com grandes clusters de espaço de armazenamento. Costumamos falar sobre big data quando usamos uma pilha com Hadoop/Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursos de Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os recursos do big data são descritos como os quatro Vs – velocidade, volume, variedade e veracidade.\n",
    "A Tabela 2-1 destaca esses recursos do big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela 2-1. Recursos de Big Data\n",
    "\n",
    "| Elemento | Descrição |\n",
    "|---|---|\n",
    "|Velocidade | Tecnologias modernas e conectividade aprimorada permitem gerar dados em uma velocidade velocidade sem precedentes. As características de velocidade incluem dados em lote, dados em tempo próximo ou em tempo real e fluxos. |\n",
    "| Volume | A escala na qual os dados aumentam. A natureza das fontes de dados e infraestrutura influenciar o volume de dados. As características do volume incluem exabyte, zetabyte, etc. |\n",
    "| Variedade | Os dados podem vir de fontes exclusivas. Os dispositivos tecnológicos modernos deixam pegadas digitais aqui e ali, o que aumenta o número de fontes de onde as empresas e as pessoas podem obter dados. As características de variedade incluem a estrutura e a complexidade dos dados. |\n",
    "| Veracidade | Os dados devem vir de fontes confiáveis. Além disso, deve ser de alta qualidade, consistente e completo. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impacto do Big Data nas empresas e nas pessoas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem dúvida, o big data afeta a forma como pensamos e fazemos negócios. As organizações orientadas por dados normalmente estabelecem a base para uma gestão baseada em evidências. Big data envolve medir os principais aspectos do negócio usando métodos quantitativos. Ajuda a apoiar a tomada de decisões. As próximas seções discutem maneiras pelas quais o big data afeta empresas e pessoas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Melhores relacionamentos com o cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os insights de big data ajudam a gerenciar o relacionamento com os clientes. As organizações com big data sobre os seus clientes podem estudar os padrões comportamentais dos clientes e utilizar análises descritivas para impulsionar estratégias de gestão de clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Desenvolvimento de produto refinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As organizações orientadas por dados usam análises de big data e análises preditivas para impulsionar o desenvolvimento de produtos. e estratégias de gestão. Essa abordagem é útil para entrega incremental e iterativa de aplicativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Melhor tomada de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando uma empresa possui big data, ela pode usá-los para descobrir padrões complexos de um fenômeno para influenciar a estratégia. Esta abordagem ajuda a gestão a tomar decisões bem informadas com base em evidências, e não em raciocínios subjetivos. As organizações orientadas por dados promovem uma cultura de gestão baseada em evidências.\n",
    "\n",
    "Também usamos big data em áreas como ciências biológicas, física, economia e medicina. Há muitas maneiras pelas quais o big data afeta o mundo. Este capítulo não considera todos os fatores. As próximas seções explicam o armazenamento de big data e as atividades de ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armazenamento de Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas últimas décadas, as organizações investiram em bancos de dados locais, incluindo Microsoft Access, Microsoft SQL Server, SAP Hana, Oracle Database e muitos mais. Recentemente, houve ampla adoção de bancos de dados em nuvem, como Microsoft Azure SQL e Oracle XE. Existem também bancos de dados padrão de big data (distribuídos), como Cassandra e HBase, entre outros. As empresas estão migrando para bancos de dados escaláveis baseados em nuvem para aproveitar os benefícios associados ao aumento do poder computacional, tecnologias tolerantes a falhas e soluções escaláveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ETL de Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora tenha havido avanços significativos no gerenciamento de bancos de dados, a forma como as pessoas manipulam os dados dos bancos de dados permanece a mesma. Extrair, transformar e carregar (ETL) ainda desempenha um papel fundamental na análise e nos relatórios. A Tabela 2-2 discute ETL Atividades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela 2-2. Atividades ETL\n",
    "\n",
    "| Atividade | Descrição |\n",
    "|---|---|\n",
    "| Extrair | Envolve a obtenção de dados de algum banco de dados. |\n",
    "| Transformar | Envolve a conversão de dados de um banco de dados em um formato adequado para análise e geração de relatórios. |\n",
    "| Carregar | Envolve o armazenamento de dados em um sistema de gerenciamento de banco de dados. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar atividades ETL, você deve usar uma linguagem de consulta. A consulta mais popular linguagem é SQL (linguagem de consulta padrão). Existem outras linguagens de consulta que surgiram com o movimento de código aberto, como HiveQL e BigQuery. A linguagem de programação Python oferece suporte a SQL. Frameworks Python podem se conectar a bancos de dados implementando bibliotecas, como SQLAlchemy, pyodbc, SQLite, SparkSQL e pandas, entre outras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estruturas de Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As estruturas de big data permitem que os desenvolvedores coletem, gerenciem e manipulem dados distribuídos. A maioria das estruturas de big data de código aberto usa computação em cluster na memória. As estruturas mais populares incluem Hadoop, Spark, Flink, Storm e Samza. Este livro usa PySpark para realizar atividades ETL, explorar dados e construir pipelines de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark executa computação em cluster na memória. Ele permite que os desenvolvedores criem aplicativos escalonáveis usando Java, Scala, Python, R e SQL. Inclui componentes de cluster como driver, gerenciador de cluster e executor. Você pode usá-lo como um gerenciador de cluster independente ou sobre Mesos, Hadoop, YARN ou Baronets. Você pode usá-lo para acessar dados no Hadoop File System (HDFS), Cassandra, HBase e Hive, entre outras fontes de dados. A estrutura de dados Spark é considerada um conjunto de dados distribuído resiliente. Este livro apresenta uma estrutura que integra Python e Apache Spark (PySpark). O livro o usa para operar o Spark MLlib. Para entender essa estrutura, primeiro você precisa compreender a ideia por trás dos conjuntos de dados distribuídos resilientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conjuntos de dados distribuídos resilientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjuntos de dados distribuídos resilientes (RDDs) são elementos imutáveis para paralelizar dados ou para transformar dados existentes. As principais operações de RDD incluem transformação e ações. Nós os armazenamos em qualquer armazenamento compatível com Hadoop. Por exemplo, em um sistema de arquivos distribuídos Hadoop (HDF), Cassandra, HBase, Amazon S3, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuração do Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As áreas de configuração do Spark incluem propriedades do Spark, variáveis de ambiente e registro em log. O diretório de configuração padrão é SPARK_HOME/conf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode instalar a biblioteca findspark em seu ambiente usando pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findspark e instale a biblioteca pyspark usando pip install pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepara a estrutura PySpark usando a estrutura findspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark as initiate_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filepath\\\\spark-3.0.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initiate_pyspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark usando o método SparkConf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "pyspark_configuration = SparkConf().setAppName(\"pyspark_linear_method\").setMaster(\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "pyspark_session = SparkSession(pyspark_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estruturas Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As estruturas Spark estendem o núcleo da API Spark. Existem quatro estruturas principais do Spark – SparkSQL, Spark Streaming, Spark MLlib e GraphX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL permite usar linguagens de consulta relacional como SQL, HiveQL e Scala. Inclui um esquemaRDD que possui objetos de linha e esquema. Você o cria usando um RDD, um arquivo parquet ou um conjunto de dados JSON existente. Você executa o Spark Context para criar um SQL contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark streaming é uma estrutura de streaming escalonável que oferece suporte a Apache Kafka, Apache Flume, HDFS e Apache Kensis, etc. Ele processa dados de entrada usando DStream em pequenos lotes que você envia usando HDFS, bancos de dados e painéis. Versões recentes do Python não oferecem suporte ao Spark Streaming. Conseqüentemente, não cobrimos a estrutura neste livro. Você pode usar um aplicativo Spark Streaming para ler a entrada de qualquer fonte de dados e armazenar uma cópia dos dados no HDFS. Isso permite que você crie e inicie um aplicativo Spark Streaming que processe os dados recebidos e execute um algoritmo neles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib é uma estrutura de ML que permite desenvolver e testar modelos de ML e DL. Em Python, os frameworks trabalham lado a lado com o framework NumPy. O Spark MLlib pode ser usado com várias fontes de dados do Hadoop e incorporado junto com os fluxos de trabalho do Hadoop. Algoritmos comuns incluem regressão, classificação, clustering, filtragem colaborativa e redução de dimensão. Os principais utilitários de fluxo de trabalho incluem transformação de recursos, padronização e normalização, desenvolvimento de pipeline, avaliação de modelo e otimização de hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphX é uma estrutura escalável e tolerante a falhas para computação paralela gráfica iterativa e rápida, redes sociais e modelagem de linguagem. Inclui algoritmos gráficos como PageRank para estimar a importância de cada vértice em um gráfico, Componentes conectados para rotular componentes conectados do gráfico com o ID de seu vértice de numeração mais baixa e Contagem de triângulos para encontrar o número de triângulos que passam por cada vértice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estruturas de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver problemas de ML, você precisa ter uma estrutura que suporte a construção e o dimensionamento de modelos de ML. Não faltam modelos de ML – existem inúmeras estruturas para ML. Existem várias estruturas de ML que você pode usar. Os capítulos subsequentes cobrem estruturas como Scikit-Learn, Spark MLlib, H2O e XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estrutura Scikit-Learn inclui algoritmos de ML como regressão, classificação e clustering, entre outros. Você pode usá-lo com outras estruturas, como NumPy e SciPy. Ele pode executar a maioria das tarefas necessárias para projetos de ML, como processamento de dados, transformação, divisão de dados, normalização, otimização de hiperparâmetros, desenvolvimento de modelo e avaliação. Scikit-Learn vem com a maioria dos pacotes de distribuição que suportam Python. Use pip install sklearn para instalá-lo em seu ambiente Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H2O é uma estrutura de ML que usa tecnologia sem driver. Ele permite acelerar a adoção de soluções de IA. É muito fácil de usar e não requer nenhum conhecimento técnico. Não só isso, mas também suporta dados numéricos e categóricos, incluindo texto. Antes de treinar o modelo de ML, primeiro você deve carregar os dados no cluster H2O. Suporta arquivos CSV, Excel e Parquet. As fontes de dados padrão incluem sistemas de arquivos locais, arquivos remotos, Amazon S3, HDFS, etc. Possui algoritmos de ML como regressão, classificação, análise de cluster e redução de dimensão. Ele também pode executar a maioria das tarefas necessárias para projetos de ML, como processamento de dados, transformação, divisão de dados, normalização, otimização de hiperparâmetros, desenvolvimento de modelo, verificação de apontamento, avaliação e produção. Use pip install h2o para instalar o pacote em seu ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando a Estrutura H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 21.0.2+13-LTS-58, mixed mode, sharing)\n",
      "  Starting server from C:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\CAQUEM~1\\AppData\\Local\\Temp\\tmpdlnh7u3b\n",
      "  JVM stdout: C:\\Users\\CAQUEM~1\\AppData\\Local\\Temp\\tmpdlnh7u3b\\h2o_cmiranda_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\CAQUEM~1\\AppData\\Local\\Temp\\tmpdlnh7u3b\\h2o_cmiranda_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Sao_Paulo</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.44.0.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 23 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_cmiranda_fyed2t</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>2.927 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.5 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       America/Sao_Paulo\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.44.0.3\n",
       "H2O_cluster_version_age:    1 month and 23 days\n",
       "H2O_cluster_name:           H2O_from_python_cmiranda_fyed2t\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    2.927 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.5 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost é uma estrutura de ML que oferece suporte a linguagens de programação, incluindo Python. Ele executa modelos com gradiente aprimorado que são escalonáveis e aprende computação paralela e distribuída rapidamente sem sacrificar a eficiência da memória. Não só isso, mas é um aluno conjunto. Conforme mencionado no Capítulo a, os alunos em conjunto podem resolver problemas de regressão e de classificação. O XGBoost usa boosting para aprender com os erros cometidos nas árvores anteriores. É útil quando modelos baseados em árvore são superajustados. Use pip install xgboost para instalar o modelo em seu ambiente Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estruturas DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As estruturas DL fornecem uma estrutura que suporta o dimensionamento de redes neurais artificiais. Você pode usá-lo sozinho ou com outros modelos. Normalmente inclui programas e estruturas de código. As estruturas DL primárias incluem TensorFlow, PyTorch, Deeplearning4j, Microsoft Cognitive Toolkit (CNTK) e Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras é uma estrutura DL de alto nível escrita em Python; ele é executado em uma plataforma de ML conhecida como TensorFlow. É eficaz para prototipagem rápida de modelos DL. Você pode executar Keras em unidades de processamento tensor ou em grandes unidades de processamento gráfico. As principais APIs Keras incluem modelos, camadas e retornos de chamada. O Capítulo 7 cobre esta estrutura. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute pip install Keras e pip install tensorflow para usar a estrutura Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Keras\n",
    "#!pip install tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
